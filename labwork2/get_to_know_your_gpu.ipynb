{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyWNUa6bJTQo",
        "outputId": "5e4f7ca7-a852-418f-b63d-00747acefd5a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OTIcfhhNJS9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numba\n",
        "from numba import cuda\n",
        "\n",
        "# Detect all GPUs (Optional if you have more than one GPU)\n",
        "cuda.detect()\n",
        "\n",
        "# Select the first GPU (if multiple GPUs are present, adjust the index accordingly)\n",
        "device = cuda.select_device(0)  # You can choose any device ID as per your need\n",
        "\n",
        "# Print Device Information\n",
        "print(f\"Device Name: {device.name}\")\n",
        "\n",
        "# Core Information\n",
        "print(f\"Multiprocessor Count: {device.MULTIPROCESSOR_COUNT}\")\n",
        "print(f\"Total Cores (Estimated): {device.MULTIPROCESSOR_COUNT * 64}\")  # 64 is a common number of cores per SM, may vary for other GPUs\n",
        "\n",
        "\n",
        "# Hint: You can also retrieve more device properties using the device attributes\n",
        "print(\"\\nDetailed Attributes:\")\n",
        "for attribute in dir(device):\n",
        "    if not attribute.startswith(\"_\"):  # Skip internal properties\n",
        "        print(f\"{attribute}: {getattr(device, attribute)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC7MWdGSKI_z",
        "outputId": "7affc22e-0482-413c-97ab-745f880172df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 CUDA devices\n",
            "id 0             b'Tesla T4'                              [SUPPORTED]\n",
            "                      Compute Capability: 7.5\n",
            "                           PCI Device ID: 4\n",
            "                              PCI Bus ID: 0\n",
            "                                    UUID: GPU-1db530b7-d509-2cd0-bd55-24d2dd4e3db7\n",
            "                                Watchdog: Disabled\n",
            "             FP32/FP64 Performance Ratio: 32\n",
            "Summary:\n",
            "\t1/1 devices are supported\n",
            "Device Name: b'Tesla T4'\n",
            "Multiprocessor Count: 40\n",
            "Total Cores (Estimated): 2560\n",
            "\n",
            "Detailed Attributes:\n",
            "COMPUTE_CAPABILITY_MAJOR: 7\n",
            "COMPUTE_CAPABILITY_MINOR: 5\n",
            "KERNEL_EXEC_TIMEOUT: 0\n",
            "MULTIPROCESSOR_COUNT: 40\n",
            "PCI_BUS_ID: 0\n",
            "PCI_DEVICE_ID: 4\n",
            "SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO: 32\n",
            "TCC_DRIVER: 0\n",
            "attributes: {}\n",
            "compute_capability: (7, 5)\n",
            "from_identity: <bound method Device.from_identity of <class 'numba.cuda.cudadrv.driver.Device'>>\n",
            "get_device_identity: <bound method Device.get_device_identity of <CUDA device 0 'b'Tesla T4''>>\n",
            "get_primary_context: <bound method Device.get_primary_context of <CUDA device 0 'b'Tesla T4''>>\n",
            "id: 0\n",
            "name: b'Tesla T4'\n",
            "primary_context: <CUDA context c_void_p(94669588235824) of device 0>\n",
            "release_primary_context: <bound method Device.release_primary_context of <CUDA device 0 'b'Tesla T4''>>\n",
            "reset: <bound method Device.reset of <CUDA device 0 'b'Tesla T4''>>\n",
            "supports_float16: True\n",
            "uuid: GPU-1db530b7-d509-2cd0-bd55-24d2dd4e3db7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "\n",
        "# Get the current device\n",
        "device = cuda.get_current_device()\n",
        "\n",
        "# Get memory information using `get_memory_info`\n",
        "free_mem, total_mem = cuda.current_context().get_memory_info()\n",
        "\n",
        "# Convert to megabytes (optional)\n",
        "total_mem_MB = total_mem / (1024 ** 2)\n",
        "\n",
        "print(f\"Total memory: {total_mem_MB:.2f} MB\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm6OmoJqL_Vl",
        "outputId": "6b601470-6cb3-4267-8217-8d2a55590c24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total memory: 15102.06 MB\n"
          ]
        }
      ]
    }
  ]
}